# -*- coding: utf-8 -*-
"""Wikipedia-Scrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YgMZlYJYAROds7PmkmSmx-mm8kup9mIG

# Web Scraping - Wikipedia
### **Overview**: Mengambil data dari tabel **Daftar kota di Indonesia Berdasarkan Kepadatan Penduduk** yang terdapat di halaman Wikipedia.

### 1. Import Library/Packages
"""

from bs4 import BeautifulSoup
import requests

"""### Define URL of the website to *scrape*"""

url = 'https://id.wikipedia.org/wiki/Daftar_kota_di_Indonesia_menurut_kepadatan_penduduk'

"""### 2. Send an HTTP GET request to the website"""

page = requests.get(url)
soup = BeautifulSoup(page.text, 'html')

"""### 3. Find the required data"""

# The table that want to scrape is at index 1

table = soup.find_all('table')[1]

# karena judul kolom table berada dalam tag <th>...<\th>

indonesia_title = table.find_all('th')
indonesia_title

indonesia_title_table = [title.text.strip() for title in indonesia_title]
print (indonesia_title_table)

# Create the Data Frame
import pandas as pd

df = pd.DataFrame(columns = indonesia_title_table)
df

column_data = table.find_all('tr')

for row in column_data[1:]:
  row_data = row.find_all('td')
  individual_row_data = [data.text.strip() for data in row_data]
  # print(individual_row_data)

  length = len(df)
  df.loc[length] = individual_row_data

df

# Mengganti beberapa nama kolom dan Menghapus kolom ref

df.rename(columns = {'Luas wilayah(Km2)': 'Luas_Wilayah', 'Jumlahpenduduk[1]': 'Jumlah_Penduduk', 'Kepadatanper km2': 'Kepadatan_Penduduk' }, inplace = True)
df.drop(columns =['Ref.'], inplace=True)

df.head()

"""### 4. Save the data into .csv file"""

df.to_csv('data_kepadatan_penduduk_indonesia.csv', index=False)
from google.colab import files
files.download('data_kepadatan_penduduk_indonesia.csv')